{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "learnseq2seq.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUAv2XHdNmbi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cd ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqxGLluNObvz",
    "outputId": "df615fec-2287-42ed-baee-ed727aa71a0b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
      "    from kaggle.cli import main\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
      "    api.authenticate()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 166, in authenticate\n",
      "    self.config_file, self.config_dir))\n",
      "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download sunnysai12345/news-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8fgpczvO4ZG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4e5d333a-17b1-4c75-9c23-1ff7de892617",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unzip:  cannot find or open /content/news-summary.zip, /content/news-summary.zip.zip or /content/news-summary.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!unzip \"/content/news-summary.zip\" -d \"/content/drive/MyDrive/DATASETS/NEWS\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = pd.read_csv('./news_summary.csv',\n",
    "                      encoding='iso-8859-1')\n",
    "raw = pd.read_csv('./news_summary_more.csv',\n",
    "                  encoding='iso-8859-1')"
   ],
   "metadata": {
    "id": "j_lyM-u3sBw8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pre1 = raw.iloc[:, 0:2].copy()\n",
    "pre2 = summary.iloc[:, 0:6].copy()\n",
    "\n",
    "pre2['text'] = pre2['author'].str.cat(pre2['date'\n",
    "        ].str.cat(pre2['read_more'].str.cat(pre2['text'\n",
    "        ].str.cat(pre2['ctext'], sep=' '), sep=' '), sep=' '), sep=' ')\n",
    "\n",
    "pre = pd.DataFrame()\n",
    "pre['text'] = pd.concat([pre1['text'], pre2['text']], ignore_index=True)\n",
    "pre['summary'] = pd.concat([pre1['headlines'], pre2['headlines']],\n",
    "                           ignore_index=True)\n",
    "\n",
    "pre.head(2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "C12i11c4tPRe",
    "outputId": "f40c200a-77cf-4167-f7a3-6ce5f9ffa742",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0  Saurav Kant, an alumnus of upGrad and IIIT-B's...   \n1  Kunal Shah's credit card bill payment platform...   \n\n                                             summary  \n0  upGrad learner switches to career in ML & Al w...  \n1  Delhi techie wins free food from Swiggy for on...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Kunal Shah's credit card bill payment platform...</td>\n      <td>Delhi techie wins free food from Swiggy for on...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Remove non-alphabetic characters (Data Cleaning)\n",
    "def text_strip(column):\n",
    "\n",
    "    for row in column:\n",
    "        row = re.sub(\"(\\\\t)\", \" \", str(row)).lower()\n",
    "        row = re.sub(\"(\\\\r)\", \" \", str(row)).lower()\n",
    "        row = re.sub(\"(\\\\n)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove _ if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(__+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove - if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(--+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove ~ if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(~~+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove + if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(\\+\\++)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove . if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(\\.\\.+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove the characters - <>()|&©ø\"',;?~*!\n",
    "        row = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove mailto:\n",
    "        row = re.sub(\"(mailto:)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove \\x9* in text\n",
    "        row = re.sub(r\"(\\\\x9\\d)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Replace INC nums to INC_NUM\n",
    "        row = re.sub(\"([iI][nN][cC]\\d+)\", \"INC_NUM\", str(row)).lower()\n",
    "\n",
    "        # Replace CM# and CHG# to CM_NUM\n",
    "        row = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", \"CM_NUM\", str(row)).lower()\n",
    "\n",
    "        # Remove punctuations at the end of a word\n",
    "        row = re.sub(\"(\\.\\s+)\", \" \", str(row)).lower()\n",
    "        row = re.sub(\"(\\-\\s+)\", \" \", str(row)).lower()\n",
    "        row = re.sub(\"(\\:\\s+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Replace any url to only the domain name\n",
    "        try:\n",
    "            url = re.search(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", str(row))\n",
    "            repl_url = url.group(3)\n",
    "            row = re.sub(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", repl_url, str(row))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Remove multiple spaces\n",
    "        row = re.sub(\"(\\s+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove the single character hanging between any two spaces\n",
    "        row = re.sub(\"(\\s+.\\s+)\", \" \", str(row)).lower()\n",
    "\n",
    "        yield row"
   ],
   "metadata": {
    "id": "CxGAhJhatY2h",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "processed_text = text_strip(pre['text'])\n",
    "processed_summary = text_strip(pre['summary'])"
   ],
   "metadata": {
    "id": "HqIyo-fvtck-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from time import time\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) \n",
    "\n",
    "text = [str(doc) for doc in nlp.pipe(processed_text, batch_size=50)]\n",
    "\n",
    "summary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(processed_summary, batch_size=50)]"
   ],
   "metadata": {
    "id": "0nAIEoiKtfZL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text[0]"
   ],
   "metadata": {
    "id": "tO2lxKqvtkAc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "outputId": "95b21192-88b0-4b9f-af89-9356c9499911",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "'saurav kant an alumnus of upgrad and iiit-b pg program in machine learning and artificial intelligence was sr systems engineer at infosys with almost years of work experience the program and upgrad 360-degree career support helped him transition to data scientist at tech mahindra with 90% salary hike upgrad online power learning has powered lakh+ careers.'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "summary[0]"
   ],
   "metadata": {
    "id": "nA9Xy5YgtmZ2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "57bc6046-f1eb-4f41-ee0c-e5073aaa388e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'_START_ upgrad learner switches to career in ml al with 90% salary hike _END_'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pre['cleaned_text'] = pd.Series(text)\n",
    "pre['cleaned_summary'] = pd.Series(summary)"
   ],
   "metadata": {
    "id": "sxGcHMBWtqz0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_count = []\n",
    "summary_count = []\n",
    "\n",
    "for sent in pre['cleaned_text']:\n",
    "    text_count.append(len(sent.split()))\n",
    "    \n",
    "for sent in pre['cleaned_summary']:\n",
    "    summary_count.append(len(sent.split()))\n",
    "\n",
    "graph_df = pd.DataFrame() \n",
    "\n",
    "graph_df['text'] = text_count\n",
    "graph_df['summary'] = summary_count\n",
    "\n",
    "graph_df.hist(bins = 5)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "R5tZqx0Wtt2z",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "outputId": "0b944d5b-ba8d-41a1-d919-a1712a623f49",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cnt = 0\n",
    "for i in pre['cleaned_text']:\n",
    "    if len(i.split()) <= 100:\n",
    "        cnt = cnt + 1\n",
    "print(cnt / len(pre['cleaned_text']))"
   ],
   "metadata": {
    "id": "NX-TSXKVtvvN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f38657eb-bf86-4e46-c5aa-7ce057aca738",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9578389933440218\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "max_text_len = 100\n",
    "max_summary_len = 15"
   ],
   "metadata": {
    "id": "DVPGBwrftwzc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "cleaned_text = np.array(pre['cleaned_text'])\n",
    "cleaned_summary= np.array(pre['cleaned_summary'])\n",
    "\n",
    "short_text = []\n",
    "short_summary = []\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_text[i].split()) <= max_text_len:\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "post_pre = pd.DataFrame({'text': short_text,'summary': short_summary})\n",
    "\n",
    "post_pre.head(2)"
   ],
   "metadata": {
    "id": "TttJRVk4tzCn",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "outputId": "b6717bf2-8a82-437f-b858-9d48fa22bd3d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0  saurav kant an alumnus of upgrad and iiit-b pg...   \n1  kunal shah credit card bill payment platform c...   \n\n                                             summary  \n0  _START_ upgrad learner switches to career in m...  \n1  _START_ delhi techie wins free food from swigg...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>saurav kant an alumnus of upgrad and iiit-b pg...</td>\n      <td>_START_ upgrad learner switches to career in m...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>kunal shah credit card bill payment platform c...</td>\n      <td>_START_ delhi techie wins free food from swigg...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "post_pre['summary'] = post_pre['summary'].apply(lambda x: 'sostok ' + x \\\n",
    "        + ' eostok')\n",
    "\n",
    "post_pre.head(2)"
   ],
   "metadata": {
    "id": "f8ylBdAut1Gc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "outputId": "abb323e7-3c9d-4d2d-acb8-fb54bd132899",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0  saurav kant an alumnus of upgrad and iiit-b pg...   \n1  kunal shah credit card bill payment platform c...   \n\n                                             summary  \n0  sostok _START_ upgrad learner switches to care...  \n1  sostok _START_ delhi techie wins free food fro...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>saurav kant an alumnus of upgrad and iiit-b pg...</td>\n      <td>sostok _START_ upgrad learner switches to care...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>kunal shah credit card bill payment platform c...</td>\n      <td>sostok _START_ delhi techie wins free food fro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(\n",
    "    np.array(post_pre[\"text\"]),\n",
    "    np.array(post_pre[\"summary\"]),\n",
    "    test_size=0.01,\n",
    "    random_state=0,\n",
    "    shuffle=True,\n",
    ")"
   ],
   "metadata": {
    "id": "lF7FBelMt2fN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Tokenize the text to get the vocab count \n",
    "from keras_preprocessing.text import Tokenizer \n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Prepare a tokenizer on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ],
   "metadata": {
    "id": "4tocx4JKt5Jz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "thresh = 5\n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "\n",
    "for key, value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    if value < thresh:\n",
    "        cnt = cnt + 1\n",
    "    \n",
    "print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)"
   ],
   "metadata": {
    "id": "XaH_WsXKt6rE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "512911be-4b5d-45a0-c1d8-9b49f842fe0c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary:  62.54196410175613\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "# Pad zero upto maximum length\n",
    "x_tr = pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "x_voc = x_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in X = {}\".format(x_voc))"
   ],
   "metadata": {
    "id": "7VrOPW7muA0E",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8554f637-a04b-4b51-a751-2bd44e76affb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in X = 30908\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare a tokenizer on testing data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "thresh = 5\n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "\n",
    "for key, value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    if value < thresh:\n",
    "        cnt = cnt + 1\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
    "\n",
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "# Pad zero upto maximum length\n",
    "y_tr = pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "y_voc = y_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ],
   "metadata": {
    "id": "B5LuFEyVuC2X",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1ad63a7e-9dd7-479d-e2ce-79dcc8f2f874",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 62.151629002411255\n",
      "Size of vocabulary in Y = 13500\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens\n",
    "ind = []\n",
    "\n",
    "for i in range(len(y_tr)):\n",
    "    cnt = 0\n",
    "    for j in y_tr[i]:\n",
    "        if j != 0:\n",
    "            cnt = cnt + 1\n",
    "    if cnt == 2:\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr = np.delete(y_tr, ind, axis=0)\n",
    "x_tr = np.delete(x_tr, ind, axis=0)"
   ],
   "metadata": {
    "id": "zc3VA8PWuCtz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ind = []\n",
    "for i in range(len(y_val)):\n",
    "    cnt = 0\n",
    "    for j in y_val[i]:\n",
    "        if j != 0:\n",
    "            cnt = cnt + 1\n",
    "    if cnt == 2:\n",
    "        ind.append(i)\n",
    "  \n",
    "y_val = np.delete(y_val, ind, axis=0)\n",
    "x_val = np.delete(x_val, ind, axis=0)"
   ],
   "metadata": {
    "id": "Ij0A7KNguHSk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# New Section"
   ],
   "metadata": {
    "id": "Ogjv7zUH3df1",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, Embedding, Dense, \\\n",
    "    Concatenate, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping"
   ],
   "metadata": {
    "id": "kgH1CO25uI0k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "latent_dim = 300\n",
    "embedding_dim = 200\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len, ))\n",
    "\n",
    "enc_emb = Embedding(x_voc, embedding_dim,\n",
    "                    trainable=True)(encoder_inputs)\n",
    "\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
    "                     return_state=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
    "\n",
    "\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
    "                     return_state=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
    "\n",
    "\n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
    "                     return_sequences=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using encoder_states as the initial state\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
    "                    return_state=True, dropout=0.4,\n",
    "                    recurrent_dropout=0.2)\n",
    "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
    "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()\n"
   ],
   "metadata": {
    "id": "XeK3DLoAuRE7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "72f2a240-4636-49e9-b7d9-220beb379270",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 100, 200)     6181600     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 100, 300),   601200      ['embedding[0][0]']              \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 100, 300),   721200      ['lstm[0][0]']                   \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 200)    2700000     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 100, 300),   721200      ['lstm_1[0][0]']                 \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 300),  601200      ['embedding_1[0][0]',            \n",
      "                                 (None, 300),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 300)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 13500)  4063500    ['lstm_3[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,589,900\n",
      "Trainable params: 15,589,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ],
   "metadata": {
    "id": "6ENk6hZYuTX0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import keras\n",
    "model= keras.models.load_model(\"./ls2s\")"
   ],
   "metadata": {
    "id": "IN9bX7r3v5sE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# New Section"
   ],
   "metadata": {
    "id": "4kAvp5Mx_YN_",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "crbLkXhIuX2l",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "d097045b-91a0-48ca-aedc-4b59fd8b22ec",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_1/lstm_4/transpose_1' defined at (most recent call last):\n    File \"C:\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Zaid\\AppData\\Local\\Temp\\ipykernel_14344\\3442206250.py\", line 1, in <cell line: 1>\n      history = model.fit(\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py\", line 515, in __call__\n      return super(RNN, self).__call__(inputs, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py\", line 574, in call\n      last_output, outputs, states = backend.rnn(\n    File \"C:\\Python310\\lib\\site-packages\\keras\\backend.py\", line 4805, in rnn\n      outputs = tf.nest.map_structure(swap_batch_timestep, outputs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\backend.py\", line 4473, in swap_batch_timestep\n      return tf.compat.v1.transpose(input_t, axes)\nNode: 'model_1/lstm_4/transpose_1'\nOOM when allocating tensor with shape[768,100,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_1/lstm_4/transpose_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_30258]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Input \u001B[1;32mIn [30]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43mx_tr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_tr\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_tr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_tr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_tr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#callbacks=[es],\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m768\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mx_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                     \u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                     \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: Graph execution error:\n\nDetected at node 'model_1/lstm_4/transpose_1' defined at (most recent call last):\n    File \"C:\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Zaid\\AppData\\Local\\Temp\\ipykernel_14344\\3442206250.py\", line 1, in <cell line: 1>\n      history = model.fit(\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py\", line 515, in __call__\n      return super(RNN, self).__call__(inputs, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py\", line 574, in call\n      last_output, outputs, states = backend.rnn(\n    File \"C:\\Python310\\lib\\site-packages\\keras\\backend.py\", line 4805, in rnn\n      outputs = tf.nest.map_structure(swap_batch_timestep, outputs)\n    File \"C:\\Python310\\lib\\site-packages\\keras\\backend.py\", line 4473, in swap_batch_timestep\n      return tf.compat.v1.transpose(input_t, axes)\nNode: 'model_1/lstm_4/transpose_1'\nOOM when allocating tensor with shape[768,100,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_1/lstm_4/transpose_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_30258]"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [x_tr, y_tr[:, :-1]],\n",
    "    y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],\n",
    "    epochs=3,\n",
    "    #callbacks=[es],\n",
    "    batch_size=768,\n",
    "    validation_data=([x_val, y_val[:, :-1]],\n",
    "                     y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:\n",
    "                     , 1:]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# New Section"
   ],
   "metadata": {
    "id": "keGTo-AMZPff",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.save(\"/content/drive/MyDrive/Keras Models/ls2s\")"
   ],
   "metadata": {
    "id": "2CPJoQAXwdwl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import keras\n",
    "model= keras.models.load_model(\"/content/drive/MyDrive/Keras Models/ls2s\")\n"
   ],
   "metadata": {
    "id": "CA9UmI9wuYkM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# New Section"
   ],
   "metadata": {
    "id": "3E2IHAj4YU5s",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Predictions\n",
    "reverse_target_word_index = y_tokenizer.index_word\n",
    "reverse_source_word_index = x_tokenizer.index_word\n",
    "target_word_index = y_tokenizer.word_index\n"
   ],
   "metadata": {
    "id": "FoLvbsf0ucAt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
    "                      state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
    "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
    "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
    "                      decoder_state_input_h, decoder_state_input_c],\n",
    "                      [decoder_outputs2] + [state_h2, state_c2])"
   ],
   "metadata": {
    "id": "zyTe8ON9uetO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def decode_sequence(input_seq):\n",
    "\n",
    "    # Encode the input as state vectors.\n",
    "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        (output_tokens, h, c) = decoder_model.predict([target_seq]\n",
    "                + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if sampled_token != 'eostok':\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find the stop word.\n",
    "        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n",
    "            >= max_summary_len - 1:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        (e_h, e_c) = (h, c)\n",
    "\n",
    "    return decoded_sentence"
   ],
   "metadata": {
    "id": "lhMf3VPyugDT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def seq2summary(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0 and i != target_word_index['sostok'] and i \\\n",
    "            != target_word_index['eostok']:\n",
    "            newString = newString + reverse_target_word_index[i] + ' '\n",
    "\n",
    "    return newString\n",
    "\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0:\n",
    "            newString = newString + reverse_source_word_index[i] + ' '\n",
    "\n",
    "    return newString"
   ],
   "metadata": {
    "id": "QSvThtDIuhcM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(30,40):\n",
    "    print('Review:', seq2text(x_tr[i]))\n",
    "    print('Original summary:', seq2summary(y_tr[i]))\n",
    "    print('Predicted summary:', decode_sequence(x_tr[i].reshape(1,\n",
    "           max_text_len)))\n",
    "    print('\\n')"
   ],
   "metadata": {
    "id": "RCT7vScCui00",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}